{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7245f9",
   "metadata": {},
   "source": [
    "\n",
    "# Text Summarization using Transformers (BART)\n",
    "\n",
    "Welcome to this project!  \n",
    "In this notebook, we’ll build a **text summarization system** using **BART (Bidirectional and Auto-Regressive Transformers)** from Hugging Face.  \n",
    "\n",
    "We’ll go through:  \n",
    "1. Loading the dataset  \n",
    "2. Preprocessing text  \n",
    "3. Fine-tuning a pretrained model  \n",
    "4. Evaluating with ROUGE metrics  \n",
    "5. Testing on real examples  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies\n",
    "!pip install transformers datasets evaluate rouge_score torch --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44195b61",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "We’ll use **Hugging Face Transformers** for the model,  \n",
    "**datasets** for data, and **evaluate** for metrics like ROUGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dffa67",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset\n",
    "We’ll use the **CNN/DailyMail dataset** which pairs news articles with human-written summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "print(dataset)\n",
    "print(\"Sample keys:\", dataset[\"train\"][0].keys())\n",
    "print(\"\\nArticle preview:\\n\", dataset[\"train\"][0][\"article\"][:500])\n",
    "print(\"\\nReference summary:\\n\", dataset[\"train\"][0][\"highlights\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba732b",
   "metadata": {},
   "source": [
    "## Step 3: Load Pretrained Model & Tokenizer\n",
    "We’ll use **facebook/bart-large-cnn**, a pretrained model specialized for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aecc93",
   "metadata": {},
   "source": [
    "## Step 4: Preprocess Data\n",
    "We need to tokenize both the **input article** and the **target summary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f103f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    targets = [doc for doc in examples[\"highlights\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, truncation=True\n",
    "    )\n",
    "\n",
    "    # Encode the targets directly (no as_target_tokenizer needed in latest transformers)\n",
    "    labels = tokenizer(\n",
    "        targets, max_length=max_target_length, truncation=True\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d1a1a",
   "metadata": {},
   "source": [
    "## Step 5: Define Evaluation Metrics\n",
    "We’ll use **ROUGE** to measure summary quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd508b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972130b5",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "We’ll fine-tune for **1 epoch** (demo purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce42914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].select(range(2000)),  # subset for demo\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e8e90",
   "metadata": {},
   "source": [
    "## Step 7: Fine-Tune the Model\n",
    "This may take time depending on hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5806480",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model\n",
    "We’ll check ROUGE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f05c599",
   "metadata": {},
   "source": [
    "##  Step 9: Test on New Example\n",
    "Now let’s test summarization on a sample article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42608e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_text = dataset[\"test\"][0][\"article\"]\n",
    "\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_length=150, \n",
    "    min_length=40, \n",
    "    num_beams=4, \n",
    "    length_penalty=2.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Original Text:\\n\", sample_text[:500], \"...\")\n",
    "print(\"\\nGenerated Summary:\\n\", tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "print(\"\\nReference Summary:\\n\", dataset[\"test\"][0][\"highlights\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90fb7f",
   "metadata": {},
   "source": [
    "## Step 10: Save and Reload Model\n",
    "Save the fine-tuned model for reuse or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"./bart-summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart-summarizer\")\n",
    "\n",
    "# Load later\n",
    "loaded_model = BartForConditionalGeneration.from_pretrained(\"./bart-summarizer\")\n",
    "loaded_tokenizer = BartTokenizer.from_pretrained(\"./bart-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99df746",
   "metadata": {},
   "source": [
    "\n",
    "# ✅ Conclusion\n",
    "- We fine-tuned **BART** for summarization  \n",
    "- Evaluated with **ROUGE metrics**  \n",
    "- Tested summarization on unseen data  \n",
    "- Saved the model for reuse or deployment  \n",
    "\n",
    "This project demonstrates an **end-to-end NLP workflow** using modern Transformers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a49d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
